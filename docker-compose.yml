version: "3.8"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    restart: unless-stopped
    environment:
      - CLUSTER_NAME=hadoop
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_replication=1
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - namenode:/hadoop/dfs/name
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
    networks:
      - smartgrid

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    hostname: datanode
    restart: unless-stopped
    depends_on:
      - namenode
    environment:
      - CLUSTER_NAME=hadoop
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - "9864:9864"
    volumes:
      - datanode:/hadoop/dfs/data
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
    networks:
      - smartgrid

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    restart: unless-stopped
    environment:
      - POSTGRES_DB=metastore
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
    volumes:
      - hive-metastore-db:/var/lib/postgresql/data
    networks:
      - smartgrid

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    depends_on:
      - hive-metastore-postgresql
      - namenode
      - datanode
    command: /opt/hive/bin/hive --service metastore
    environment:
      - SERVICE_PRECONDITION=namenode:9870 datanode:9864 hive-metastore-postgresql:5432

      # Metastore DB config
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore-postgresql:5432/metastore
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName=org.postgresql.Driver
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName=hive
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword=hive
      - HIVE_SITE_CONF_datanucleus_autoCreateSchema=false

      # HDFS FS
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - "9083:9083"
    volumes:
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
    networks:
      - smartgrid

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    depends_on:
      - hive-metastore
      - hive-metastore-postgresql
    command: /opt/hive/bin/hive --service hiveserver2
    environment:
      - SERVICE_PRECONDITION=hive-metastore:9083

      - HIVE_SITE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore-postgresql:5432/metastore
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName=org.postgresql.Driver
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName=hive
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword=hive
      - HIVE_SITE_CONF_datanucleus_autoCreateSchema=false

      - HIVE_SITE_CONF_hive_metastore_uris=thrift://hive-metastore:9083
      - HIVE_SITE_CONF_hive_server2_enable_doAs=false
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - "10000:10000"
    volumes:
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
    networks:
      - smartgrid

  spark-master:
    image: spark:3.5.3-python3
    container_name: spark-master
    command: [ "/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "spark-master", "--port", "7077", "--webui-port", "8080" ]
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./spark:/opt/spark/jobs:ro
      - ./config/hadoop/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    networks:
      - smartgrid

  spark-worker:
    image: spark:3.5.3-python3
    container_name: spark-worker
    depends_on:
      - spark-master
    command: [ "/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077" ]
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
    volumes:
      - ./config/hadoop/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    networks:
      - smartgrid

  spark-client:
    image: spark:3.5.3-python3
    container_name: spark-client
    depends_on:
      - spark-master
    entrypoint: [ "bash", "-c", "tail -f /dev/null" ]
    volumes:
      - ./spark:/opt/spark/jobs:ro
      - ./config/hadoop/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    networks:
      - smartgrid

  trino:
    image: trinodb/trino:418
    container_name: trino
    depends_on:
      - hive-metastore
    environment:
      - JAVA_TOOL_OPTIONS=-Dhive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
    ports:
      - "8085:8080"
    volumes:
      - ./config/trino/catalog:/etc/trino/catalog:ro
      - ./config/hadoop:/etc/hadoop/conf:ro
    networks:
      - smartgrid

  grafana:
    image: grafana/grafana:10.4.3
    container_name: grafana
    depends_on:
      - trino
    environment:
      # correct plugin id
      - GF_INSTALL_PLUGINS=trino-datasource
      # allow unsigned plugin just in case
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=trino-datasource
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
    ports:
      - "3001:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - smartgrid

  simulator:
    build:
      context: ./simulator
    container_name: simulator
    depends_on:
      - namenode
    environment:
      - PYTHONUNBUFFERED=1
    volumes:
      - ./data/raw:/data/raw
      - ./data/state:/data/state
    command: [ "--output-dir", "/data/raw", "--metadata-file", "/data/state/state.json", "--hdfs-url", "http://namenode:9870", "--hdfs-directory", "/user/dr.who/smartmeter" ]
    networks:
      - smartgrid

networks:
  smartgrid:
    driver: bridge

volumes:
  namenode:
  datanode:
  hive-metastore-db:
  grafana-data:
